{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import sacrebleu\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments,\n",
    "    MarianMTModel, MarianTokenizer, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        return device\n",
    "    else:\n",
    "        print(\"No GPU available, using CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def load_balanced_data(path, sample_size=10000):\n",
    "    columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "    df = pd.read_csv(path, encoding='latin-1', names=columns)\n",
    "    pos_samples = df[df['target'] == 4].sample(n=sample_size//2, random_state=42)\n",
    "    neg_samples = df[df['target'] == 0].sample(n=sample_size//2, random_state=42)\n",
    "    return pd.concat([pos_samples, neg_samples]).reset_index(drop=True)\n",
    "\n",
    "def load_model(lang_code, device):\n",
    "    model_dir = f\"./T5-{lang_code}-en\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_sentiment_batch(texts, model, tokenizer, device, batch_size=32):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model.generate(**inputs)\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            batch_predictions = [4 if 'positive' in pred.lower() else 0 for pred in decoded]\n",
    "            predictions.extend(batch_predictions)\n",
    "            \n",
    "            # Clear CUDA cache periodically\n",
    "            if device.type == \"cuda\" and i % (batch_size * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(lang_code, test_data, device):\n",
    "    model, tokenizer = load_model(lang_code, device)\n",
    "    predictions = predict_sentiment_batch(test_data['text'].tolist(), model, tokenizer, device)\n",
    "    \n",
    "    accuracy = accuracy_score(test_data['target'], predictions)\n",
    "    report = classification_report(test_data['target'], predictions)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'accuracy': accuracy,\n",
    "        'report': report\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluating the T5 models on sentiment analysis \n",
    "# Check and set up GPU\n",
    "device = check_gpu()\n",
    "\n",
    "# Load balanced dataset\n",
    "data = load_balanced_data('training.1600000.processed.noemoticon.csv', sample_size=10000)\n",
    "print(f\"Loaded {len(data)} tweets with distribution:\\n{data['target'].value_counts()}\")\n",
    "\n",
    "# Languages to test\n",
    "languages = ['fr', 'de', 'ro']\n",
    "\n",
    "# Evaluate each model\n",
    "for lang_code in languages:\n",
    "    print(f\"\\nEvaluating {lang_code} model...\")\n",
    "    \n",
    "    try:\n",
    "        results = evaluate_model(lang_code, data, device)\n",
    "        print(f\"\\nAccuracy: {results['accuracy']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(results['report'])\n",
    "        \n",
    "        pd.DataFrame({\n",
    "            'text': data['text'],\n",
    "            'true_sentiment': data['target'],\n",
    "            'predicted_sentiment': results['predictions']\n",
    "        }).to_csv(f'sentiment_results_{lang_code}.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lang_code} model: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Clear GPU memory after each model\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roberta-Base Model \n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_name=\"roberta-base\", device=None):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=2,\n",
    "            hidden_dropout_prob=0.3,\n",
    "            attention_probs_dropout_prob=0.3\n",
    "        ).to(self.device)\n",
    "\n",
    "    def train(self, train_texts, train_labels, val_texts, val_labels, batch_size=32, epochs=3, learning_rate=2e-5):\n",
    "       train_dataset = SentimentDataset(train_texts, train_labels, self.tokenizer)\n",
    "       train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "       \n",
    "       val_dataset = SentimentDataset(val_texts, val_labels, self.tokenizer)\n",
    "       val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "       \n",
    "       # Compute class weights\n",
    "       class_counts = np.bincount(train_labels)\n",
    "       class_weights = torch.FloatTensor(1.0 / class_counts).to(self.device)\n",
    "       \n",
    "       # Use weighted loss\n",
    "       criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "       \n",
    "       # Reduce learning rate and add weight decay\n",
    "       optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "       \n",
    "       # Warmup and decay learning rate\n",
    "       num_training_steps = len(train_loader) * epochs\n",
    "       scheduler = get_linear_schedule_with_warmup(\n",
    "           optimizer,\n",
    "           num_warmup_steps=num_training_steps // 10,\n",
    "           num_training_steps=num_training_steps\n",
    "       )\n",
    "       \n",
    "       best_val_acc = 0\n",
    "       best_model = None\n",
    "       \n",
    "       for epoch in range(epochs):\n",
    "           self.model.train()\n",
    "           total_loss = 0\n",
    "           \n",
    "           for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "               batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "               \n",
    "               optimizer.zero_grad()\n",
    "               outputs = self.model(**batch)\n",
    "               \n",
    "               # Use weighted loss\n",
    "               logits = outputs.logits\n",
    "               labels = batch['labels']\n",
    "               loss = criterion(logits, labels)\n",
    "               \n",
    "               loss.backward()\n",
    "               torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "               optimizer.step()\n",
    "               scheduler.step()\n",
    "               \n",
    "               total_loss += loss.item()\n",
    "           \n",
    "           # Validation with detailed metrics\n",
    "           val_metrics = self.evaluate(val_texts, val_labels, batch_size)\n",
    "           print(f'Epoch {epoch + 1} - Train Loss: {total_loss/len(train_loader):.4f}')\n",
    "           print(f'Validation Accuracy: {val_metrics[\"accuracy\"]:.4f}')\n",
    "           print('Validation Report:')\n",
    "           print(classification_report(\n",
    "               val_labels, \n",
    "               self.predict(val_texts, batch_size),\n",
    "               labels=[0, 1],\n",
    "               target_names=['Negative', 'Positive']\n",
    "           ))\n",
    "           \n",
    "           if val_metrics[\"accuracy\"] > best_val_acc:\n",
    "               best_val_acc = val_metrics[\"accuracy\"]\n",
    "               best_model = copy.deepcopy(self.model.state_dict())\n",
    "       \n",
    "       # Restore best model\n",
    "       if best_model is not None:\n",
    "           self.model.load_state_dict(best_model)\n",
    "    \n",
    "    def predict(self, texts, batch_size=32):\n",
    "       self.model.eval()\n",
    "       dataset = SentimentDataset(texts, [0] * len(texts), self.tokenizer)\n",
    "       dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "       \n",
    "       predictions = []\n",
    "       with torch.no_grad():\n",
    "           for batch in dataloader:\n",
    "               batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "               outputs = self.model(**batch)\n",
    "               # Apply temperature scaling for better calibration\n",
    "               logits = outputs.logits / 1.5  # Temperature parameter\n",
    "               preds = torch.softmax(logits, dim=1)\n",
    "               predictions.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "       \n",
    "       return predictions\n",
    "    \n",
    "    def evaluate(self, texts, labels, batch_size=32):\n",
    "        predictions = self.predict(texts, batch_size)\n",
    "        report = classification_report(labels, predictions, output_dict=True)\n",
    "        return {\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"report\": report\n",
    "        }\n",
    "\n",
    "def prepare_balanced_data(filepath, total_samples=200000):\n",
    "    print(\"Loading data...\")\n",
    "    # Read initial chunk to get negative samples\n",
    "    neg_data = pd.read_csv(filepath, encoding='latin-1', \n",
    "                          names=['target', 'id', 'date', 'flag', 'user', 'text'],\n",
    "                          nrows=total_samples)\n",
    "    \n",
    "    # Skip to later part of file to get positive samples\n",
    "    skiprows = lambda x: x > total_samples and x <= 2*total_samples\n",
    "    pos_data = pd.read_csv(filepath, encoding='latin-1',\n",
    "                          names=['target', 'id', 'date', 'flag', 'user', 'text'],\n",
    "                          skiprows=skiprows)\n",
    "    \n",
    "    print(\"Class distribution in chunks:\")\n",
    "    print(\"First chunk:\", neg_data['target'].value_counts())\n",
    "    print(\"Second chunk:\", pos_data['target'].value_counts())\n",
    "    \n",
    "    samples_per_class = total_samples // 2\n",
    "    \n",
    "    # Sample from each class\n",
    "    try:\n",
    "        neg_samples = neg_data[neg_data['target'] == 0].sample(n=samples_per_class, random_state=42)\n",
    "    except ValueError:\n",
    "        print(\"Not enough negative samples in first chunk\")\n",
    "        return None, None, None\n",
    "        \n",
    "    try:\n",
    "        pos_samples = pos_data[pos_data['target'] == 4].sample(n=samples_per_class, random_state=42)\n",
    "    except ValueError:\n",
    "        print(\"Not enough positive samples in second chunk\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_data = pd.concat([neg_samples, pos_samples])\n",
    "    balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split data\n",
    "    train_data, temp_data = train_test_split(\n",
    "        balanced_data, \n",
    "        train_size=0.7,\n",
    "        random_state=42,\n",
    "        stratify=balanced_data['target']\n",
    "    )\n",
    "    \n",
    "    val_data, test_data = train_test_split(\n",
    "        temp_data,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=temp_data['target']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Training: {len(train_data)} ({train_data['target'].value_counts().to_dict()})\")\n",
    "    print(f\"Validation: {len(val_data)} ({val_data['target'].value_counts().to_dict()})\")\n",
    "    print(f\"Test: {len(test_data)} ({test_data['target'].value_counts().to_dict()})\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# # Training the Roberta-Base model \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Prepare data\n",
    "# train_data, val_data, test_data = prepare_balanced_data('training.1600000.processed.noemoticon.csv', total_samples=200000)\n",
    "# print(f\"\\nData distribution:\")\n",
    "# print(\"Train:\", train_data['target'].value_counts())\n",
    "# print(\"Validation:\", val_data['target'].value_counts())\n",
    "# print(\"Test:\", test_data['target'].value_counts())\n",
    "\n",
    "# # Train model\n",
    "# analyzer = SentimentAnalyzer(device=device)\n",
    "# best_acc = analyzer.train(\n",
    "#     train_texts=train_data['text'].tolist(),\n",
    "#     train_labels=[(1 if t == 4 else 0) for t in train_data['target']],\n",
    "#     val_texts=val_data['text'].tolist(),\n",
    "#     val_labels=[(1 if t == 4 else 0) for t in val_data['target']],\n",
    "#     batch_size=16,\n",
    "#     epochs=5\n",
    "# )\n",
    "\n",
    "# # Final evaluation\n",
    "# test_metrics = analyzer.evaluate(\n",
    "#     test_data['text'].tolist(),\n",
    "#     [(1 if t == 4 else 0) for t in test_data['target']]\n",
    "# )\n",
    "# print(\"\\nTest Set Results:\")\n",
    "# print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(test_metrics['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model state dictionary\n",
    "# torch.save(analyzer.model.state_dict(), 'sentiment_analyzer.pt')\n",
    "# analyzer.tokenizer.save_pretrained('sentiment_tokenizer')\n",
    "\n",
    "# Loading Roberta-Base model\n",
    "def load_sentiment_model(model_path='sentiment_analyzer.pt', tokenizer_path='sentiment_tokenizer'):\n",
    "    analyzer = SentimentAnalyzer()\n",
    "    analyzer.model.load_state_dict(torch.load(model_path))\n",
    "    analyzer.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhao\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating FR translations\n",
      "\n",
      "Evaluating English texts:\n",
      "\n",
      "Evaluating fr translations:\n",
      "\n",
      "Results for english:\n",
      "Accuracy: 0.9133\n",
      "Weighted F1: 0.9133\n",
      "\n",
      "Results for fr:\n",
      "Accuracy: 0.6433\n",
      "Weighted F1: 0.6429\n",
      "\n",
      "Evaluating DE translations\n",
      "\n",
      "Evaluating English texts:\n",
      "\n",
      "Evaluating de translations:\n",
      "\n",
      "Results for english:\n",
      "Accuracy: 0.9133\n",
      "Weighted F1: 0.9133\n",
      "\n",
      "Results for de:\n",
      "Accuracy: 0.6800\n",
      "Weighted F1: 0.6723\n",
      "\n",
      "Evaluating RO translations\n",
      "\n",
      "Evaluating English texts:\n",
      "\n",
      "Evaluating ro translations:\n",
      "\n",
      "Results for english:\n",
      "Accuracy: 0.9133\n",
      "Weighted F1: 0.9133\n",
      "\n",
      "Results for ro:\n",
      "Accuracy: 0.6633\n",
      "Weighted F1: 0.6620\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Roberta-Base\n",
    "class TranslationSentimentPipeline:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.translation_models = {}\n",
    "        self.analyzer = None\n",
    "        \n",
    "    def load_models(self, sentiment_path='sentiment_analyzer.pt', tokenizer_path='sentiment_tokenizer'):\n",
    "        # Load sentiment analyzer using the loader function\n",
    "        self.analyzer = load_sentiment_model(sentiment_path, tokenizer_path)\n",
    "        self.analyzer.model = self.analyzer.model.to(self.device)\n",
    "        self.analyzer.model.eval()\n",
    "        \n",
    "        # Load translation models\n",
    "        for lang in ['fr', 'de', 'ro']:\n",
    "            model = T5ForConditionalGeneration.from_pretrained(f\"./T5-{lang}-en\").to(self.device)\n",
    "            tokenizer = T5Tokenizer.from_pretrained(f\"./T5-{lang}-en\")\n",
    "            model.eval()\n",
    "            self.translation_models[lang] = (model, tokenizer)\n",
    "    \n",
    "    def translate_batch(self, texts, lang_code, batch_size=16):\n",
    "        model, tokenizer = self.translation_models[lang_code]\n",
    "        translations = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Translating {lang_code}\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs)\n",
    "                decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                translations.extend(decoded)\n",
    "        \n",
    "        return translations\n",
    "    \n",
    "    def predict_sentiment(self, texts, batch_size=16):\n",
    "        return self.analyzer.predict(texts, batch_size)\n",
    "    \n",
    "    def evaluate_pipeline(self, df_original, df_translated, lang_code):\n",
    "        results = {}\n",
    "\n",
    "        print(f\"\\nEvaluating English texts:\")\n",
    "        eng_preds = self.predict_sentiment(df_original['text'].tolist())\n",
    "        eng_report = classification_report(\n",
    "            [(1 if t == 4 else 0) for t in df_original['polarity']],\n",
    "            eng_preds, \n",
    "            output_dict=True\n",
    "        )\n",
    "        results['english'] = eng_report\n",
    "\n",
    "        print(f\"\\nEvaluating {lang_code} translations:\")\n",
    "        trans_preds = self.predict_sentiment(df_translated['text'].tolist())\n",
    "        trans_report = classification_report(\n",
    "            [(1 if t == 4 else 0) for t in df_translated['polarity']],\n",
    "            trans_preds, \n",
    "            output_dict=True\n",
    "        )\n",
    "        results[lang_code] = trans_report\n",
    "\n",
    "        return results\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "eng_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_eng.csv')\n",
    "fr_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_french.csv')\n",
    "ro_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_romanian.csv')\n",
    "de_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_german.csv')\n",
    "\n",
    "# Initialize and load models\n",
    "pipeline = TranslationSentimentPipeline(device=device)\n",
    "pipeline.load_models()\n",
    "\n",
    "# Evaluate each language\n",
    "for lang, data in [('fr', fr_data), ('de', de_data), ('ro', ro_data)]:\n",
    "    print(f\"\\nEvaluating {lang.upper()} translations\")\n",
    "    results = pipeline.evaluate_pipeline(eng_data, data, lang)\n",
    "    \n",
    "    for lang_code, report in results.items():\n",
    "        print(f\"\\nResults for {lang_code}:\")\n",
    "        print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "        print(f\"Weighted F1: {report['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Class distribution in chunks:\n",
      "First chunk: 0    200000\n",
      "Name: target, dtype: int64\n",
      "Second chunk: 4    800000\n",
      "0    600000\n",
      "Name: target, dtype: int64\n",
      "\n",
      "Final dataset sizes:\n",
      "Training: 140000 ({0: 70000, 4: 70000})\n",
      "Validation: 30000 ({0: 15000, 4: 15000})\n",
      "Test: 30000 ({0: 15000, 4: 15000})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhao\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d635fe09040d407ab077a047de329f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/8750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.4599\n",
      "Validation Accuracy: 0.8739\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.87      0.87     15000\n",
      "    Positive       0.87      0.88      0.87     15000\n",
      "\n",
      "    accuracy                           0.87     30000\n",
      "   macro avg       0.87      0.87      0.87     30000\n",
      "weighted avg       0.87      0.87      0.87     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af60a4e73294b6a89e122b217af3b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/8750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.4298\n",
      "Validation Accuracy: 0.8769\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.86      0.87     15000\n",
      "    Positive       0.86      0.89      0.88     15000\n",
      "\n",
      "    accuracy                           0.88     30000\n",
      "   macro avg       0.88      0.88      0.88     30000\n",
      "weighted avg       0.88      0.88      0.88     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cdd497d36a43b38106611e97b7a0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/8750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.4148\n",
      "Validation Accuracy: 0.8792\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.88      0.88     15000\n",
      "    Positive       0.88      0.88      0.88     15000\n",
      "\n",
      "    accuracy                           0.88     30000\n",
      "   macro avg       0.88      0.88      0.88     30000\n",
      "weighted avg       0.88      0.88      0.88     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad44f71e955489cb49398abf92a0e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/8750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.4033\n",
      "Validation Accuracy: 0.8832\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.88      0.88     15000\n",
      "    Positive       0.88      0.88      0.88     15000\n",
      "\n",
      "    accuracy                           0.88     30000\n",
      "   macro avg       0.88      0.88      0.88     30000\n",
      "weighted avg       0.88      0.88      0.88     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff608078ee0b4b458f9773a157f85474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/8750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.3931\n",
      "Validation Accuracy: 0.8847\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.89      0.89     15000\n",
      "    Positive       0.89      0.88      0.88     15000\n",
      "\n",
      "    accuracy                           0.88     30000\n",
      "   macro avg       0.88      0.88      0.88     30000\n",
      "weighted avg       0.88      0.88      0.88     30000\n",
      "\n",
      "\n",
      "Test Set Results (on small dataset):\n",
      "Accuracy: 0.8878\n",
      "\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.8845187731359069, 'recall': 0.8920666666666667, 'f1-score': 0.8882766861391396, 'support': 15000}, '1': {'precision': 0.8911377084454007, 'recall': 0.8835333333333333, 'f1-score': 0.8873192287091591, 'support': 15000}, 'accuracy': 0.8878, 'macro avg': {'precision': 0.8878282407906538, 'recall': 0.8877999999999999, 'f1-score': 0.8877979574241494, 'support': 30000}, 'weighted avg': {'precision': 0.8878282407906538, 'recall': 0.8878, 'f1-score': 0.8877979574241494, 'support': 30000}}\n"
     ]
    }
   ],
   "source": [
    "# Roberta-Sentiment Model\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", device=None):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # The model still expects 3 classes so we set the num_labels as 3\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=3\n",
    "        ).to(self.device)\n",
    "\n",
    "    def train(self, train_texts, train_labels, val_texts, val_labels, batch_size=16, epochs=3, learning_rate=1e-5):\n",
    "        # Convert labels to 0 and 1 for binary classification\n",
    "        train_labels = [0 if label == 0 else 1 for label in train_labels]\n",
    "        val_labels = [0 if label == 0 else 1 for label in val_labels]\n",
    "        \n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.tokenizer)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = SentimentDataset(val_texts, val_labels, self.tokenizer)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Calculate class weights for 2 classes\n",
    "        class_counts = np.bincount(train_labels)\n",
    "        class_weights = torch.FloatTensor(1.0 / class_counts).to(self.device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "        num_training_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_training_steps // 10,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "        best_val_acc = 0\n",
    "        best_model = None\n",
    "        patience = 2  # For early stopping\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(**batch)\n",
    "\n",
    "                logits = outputs.logits\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                # Apply softmax to the logits and use only the probabilities for the positive and negative classes\n",
    "                probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "                binary_probs = probs[:, [0, 2]]  # Assuming 0 is negative and 2 is positive\n",
    "                \n",
    "                loss = criterion(binary_probs, labels) # Calculate loss with binary probabilities\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            val_metrics = self.evaluate(val_texts, val_labels, batch_size)\n",
    "            val_acc = val_metrics[\"accuracy\"]\n",
    "            print(f'Epoch {epoch + 1} - Train Loss: {total_loss/len(train_loader):.4f}')\n",
    "            print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "            print('Validation Report:')\n",
    "            print(classification_report(\n",
    "                val_labels,\n",
    "                self.predict(val_texts, batch_size),\n",
    "                labels=[0, 1],  # Binary classification\n",
    "                target_names=['Negative', 'Positive']\n",
    "            ))\n",
    "\n",
    "            # Early stopping and model saving\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        if best_model is not None:\n",
    "            self.model.load_state_dict(best_model)\n",
    "\n",
    "    def predict(self, texts, batch_size=32):\n",
    "        self.model.eval()\n",
    "        dataset = SentimentDataset(texts, [0] * len(texts), self.tokenizer)  # Dummy labels\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Apply softmax and extract probabilities for the positive and negative classes\n",
    "                probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "                binary_probs = probs[:, [0, 2]]  # Assuming 0 is negative and 2 is positive\n",
    "                \n",
    "                # Get predictions based on binary probabilities\n",
    "                preds = torch.argmax(binary_probs, dim=1)\n",
    "                \n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, texts, labels, batch_size=32):\n",
    "        # Convert labels to 0 and 1 for evaluation\n",
    "        labels = [0 if label == 0 else 1 for label in labels]\n",
    "\n",
    "        predictions = self.predict(texts, batch_size)\n",
    "        report = classification_report(labels, predictions, output_dict=True)\n",
    "        return {\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"report\": report\n",
    "        }\n",
    "\n",
    "# # Training \n",
    "# if __name__ == \"__main__\":\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(f\"Using device: {device}\")\n",
    "\n",
    "#     # --- Rapid Experimentation with Pre-trained Sentiment Model ---\n",
    "\n",
    "#     # 1. Prepare a smaller dataset (e.g., 20,000 samples) for quick testing\n",
    "#     small_train_data, small_val_data, small_test_data = prepare_balanced_data(\n",
    "#         'training.1600000.processed.noemoticon.csv',\n",
    "#         total_samples=200000  # Reduce for faster experimentation\n",
    "#     )\n",
    "\n",
    "#     # 2. Instantiate SentimentAnalyzer with the pre-trained model\n",
    "#     sentiment_analyzer = SentimentAnalyzer(\n",
    "#         model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "#         device=device\n",
    "#     )\n",
    "    \n",
    "#     # Convert labels to 0 and 1 in the small datasets\n",
    "#     train_labels = [0 if label == 0 else 1 for label in small_train_data['target']]\n",
    "#     val_labels = [0 if label == 0 else 1 for label in small_val_data['target']]\n",
    "#     test_labels = [0 if label == 0 else 1 for label in small_test_data['target']]\n",
    "\n",
    "#     # 3. Train for a few epochs (e.g., 2) with a small batch size\n",
    "#     sentiment_analyzer.train(\n",
    "#         train_texts=small_train_data['text'].tolist(),\n",
    "#         train_labels=train_labels,\n",
    "#         val_texts=small_val_data['text'].tolist(),\n",
    "#         val_labels=val_labels,\n",
    "#         batch_size=16,\n",
    "#         epochs=5,  # Reduced for faster experimentation\n",
    "#         learning_rate=1e-5\n",
    "#     )\n",
    "\n",
    "#     # 4. Evaluate on the small test set\n",
    "#     test_metrics = sentiment_analyzer.evaluate(\n",
    "#         small_test_data['text'].tolist(),\n",
    "#         test_labels # Use converted labels for evaluation\n",
    "#     )\n",
    "#     print(\"\\nTest Set Results (on small dataset):\")\n",
    "#     print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(test_metrics['report'])\n",
    "\n",
    "#     # Save the model state dictionary and tokenizer\n",
    "#     torch.save(sentiment_analyzer.model.state_dict(), 'sentiment_analyzer.pt')\n",
    "#     sentiment_analyzer.tokenizer.save_pretrained('sentiment_tokenizer')\n",
    "\n",
    "    # # --- (Optional) Full Training ---\n",
    "    # # If the initial results are promising, you can train on the full dataset:\n",
    "    # # train_data, val_data, test_data = prepare_balanced_data('training.1600000.processed.noemoticon.csv', total_samples=200000)\n",
    "    # # sentiment_analyzer.train(...) # Train on the full data, potentially with more epochs\n",
    "\n",
    "    # # --- Translation and Sentiment Pipeline ---\n",
    "\n",
    "    # # Load data for translation pipeline\n",
    "    # eng_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_eng.csv')\n",
    "    # fr_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_french.csv')\n",
    "    # ro_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_romanian.csv')\n",
    "    # de_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_german.csv')\n",
    "\n",
    "    # # Initialize and load models for the pipeline\n",
    "    # pipeline = TranslationSentimentPipeline(device=device)\n",
    "    # pipeline.load_models('sentiment_analyzer.pt', 'sentiment_tokenizer')\n",
    "\n",
    "    # # Evaluate each language\n",
    "    # for lang, data in [('fr', fr_data), ('de', de_data), ('ro', ro_data)]:\n",
    "    #     print(f\"\\nEvaluating {lang.upper()} translations\")\n",
    "    #     results = pipeline.evaluate_pipeline(eng_data, data, lang)\n",
    "\n",
    "    #     for lang_code, report in results.items():\n",
    "    #         print(f\"\\nResults for {lang_code}:\")\n",
    "    #         print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "    #         print(f\"Weighted F1: {report['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhao\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating FR translations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12253011b314f048c1309901a9be291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393c684c454347c19d148956610fe88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for fr:\n",
      "Accuracy: 0.3600\n",
      "Weighted F1: 0.3774\n",
      "\n",
      "Evaluating DE translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f44286714441e09a34116f8f49a23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e09aadb4cc410c9635c6dcf3221840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for de:\n",
      "Accuracy: 0.3633\n",
      "Weighted F1: 0.3744\n",
      "\n",
      "Evaluating RO translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce5fb5a2236472ab79d5cd2464436ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de02c817c7246ad91b6b1208946e61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for ro:\n",
      "Accuracy: 0.3233\n",
      "Weighted F1: 0.3366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\yuhao\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Roberta-Sentiment\n",
    "class TranslationSentimentPipeline:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.translation_models = {}\n",
    "        self.analyzer = None\n",
    "        \n",
    "    def load_models(self, sentiment_path='sentiment_analyzer.pt', tokenizer_path='sentiment_tokenizer', translation_model='t5'):\n",
    "        self.analyzer = load_sentiment_model(sentiment_path, tokenizer_path)\n",
    "        self.analyzer.model = self.analyzer.model.to(self.device)\n",
    "        self.analyzer.model.eval()\n",
    "        \n",
    "        self.translation_model = translation_model\n",
    "        \n",
    "        for lang in ['fr', 'de', 'ro']:\n",
    "            if self.translation_model == 't5':\n",
    "                model = T5ForConditionalGeneration.from_pretrained(f\"T5models/T5-{lang}-en\").to(self.device)\n",
    "                tokenizer = T5Tokenizer.from_pretrained(f\"T5models/T5-{lang}-en\")\n",
    "            else:\n",
    "                model = MarianMTModel.from_pretrained(f'./marianmtmodels/marian-mt-{lang}-finetuned').to(self.device)\n",
    "                tokenizer = MarianTokenizer.from_pretrained(f'./marianmtmodels/marian-mt-{lang}-finetuned')\n",
    "            model.eval()\n",
    "            self.translation_models[lang] = (model, tokenizer)\n",
    "    \n",
    "    def translate_batch(self, texts, lang_code, batch_size=8):\n",
    "        model, tokenizer = self.translation_models[lang_code]\n",
    "        translations = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=64, num_beams=2)\n",
    "                decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                translations.extend(decoded)\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return translations\n",
    "    \n",
    "    def predict_sentiment(self, texts, batch_size=8):\n",
    "        predictions = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.analyzer.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.analyzer.model(**inputs)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                predictions.extend(preds.tolist())\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_pipeline(self, df_translated, lang_code):\n",
    "        translations = self.translate_batch(df_translated['text'].tolist(), lang_code)\n",
    "        trans_preds = self.predict_sentiment(translations)\n",
    "\n",
    "        trans_report = classification_report(\n",
    "            [(1 if t == 4 else 0) for t in df_translated['polarity']],  # Changed from 'target' to 'polarity'\n",
    "            trans_preds, \n",
    "            output_dict=True\n",
    "        )\n",
    "\n",
    "        return trans_report\n",
    "    \n",
    "# Load data \n",
    "eng_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_eng.csv')\n",
    "fr_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_french.csv') \n",
    "de_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_german.csv')\n",
    "ro_data = pd.read_csv('./sentiment_analysis_languages/sentiment140_romanian.csv')\n",
    "\n",
    "# Initialize pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline = TranslationSentimentPipeline(device=device)\n",
    "pipeline.load_models()\n",
    "\n",
    "# Evaluate each language\n",
    "for lang, data in [('fr', fr_data), ('de', de_data), ('ro', ro_data)]:\n",
    "    print(f\"\\nEvaluating {lang.upper()} translations\")\n",
    "    report = pipeline.evaluate_pipeline(data, lang)\n",
    "    \n",
    "    print(f\"\\nResults for {lang}:\")\n",
    "    print(f\"Accuracy: {report['accuracy']:.4f}\") \n",
    "    print(f\"Weighted F1: {report['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RobertaForMaskedLM']\n",
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base')\n",
    "model.load_state_dict(torch.load('sentiment_analyzer.pt'))\n",
    "\n",
    "print(model.config.architectures)\n",
    "print(f\"Number of labels: {model.num_labels}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
